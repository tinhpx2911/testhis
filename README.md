# BLOOM-LoRA
In this Project, we'll explore and apply state-of-the-art fine-tuning techniques to large language models. The techniques we'll delve into are not only robust but also resource-efficient, allowing us to perform the task of fine-tuning even on the free T4 GPUs available in a Kaggle notebook.

Among the techniques we will explore is Low-Rank Adaptation (LoRA), a novel method that has proven to be efficient and effective in adapting large pre-trained language models to specific tasks. LoRA is grounded in the hypothesis that updates to the weights during adaptation have a low "intrinsic rank", allowing us to constrain weight updates and reduce computational complexity, while preserving model performance.

Complementing LoRA, we will also engage with mixed-precision training. This technique combines different numerical precisions to perform computations, aiming to maximize the computational power of modern GPUs. Mixed-precision training can accelerate model training, reduce memory requirements, and thus enable us to train larger, more powerful models.

Finally, we will delve into distributed training, a must-know technique for handling very large models or datasets. With distributed training, we can leverage multiple GPUs or even multiple

# Dataset
The Stanford Alpaca dataset is part of a project that aims to build and share an instruction-following model called Alpaca. The dataset contains 52,000 examples used for fine-tuning the Alpaca model, with each example consisting of a unique instruction that the model should follow, an optional context or input for the task, and the corresponding output generated by the OpenAI's text-davinci-003 model. More information is available at the [Data release](https://github.com/tatsu-lab/stanford_alpaca/blob/main/README.md#data-release) and [Alpaca project page](https://crfm.stanford.edu/2023/03/13/alpaca.html).

# Model
[Hugging Face's BLOOM language model](https://bigscience.huggingface.co/blog/bloom) marks a significant step in AI research, being the largest open multilingual language model available. With 176 billion parameters, BLOOM has the ability to generate text in 46 natural languages and 13 programming languages, including Spanish, French, and Arabic, among others.

BLOOM represents the cumulative work of over 1000 researchers from more than 70 countries and 250 institutions. The model was trained on the Jean Zay supercomputer over a span of 117 days.

For our purposes, we will be fine-tuning the BLOOM-1.7B (1.7 billion parameters) variant, using the Stanford Alpaca dataset. This  Project will allow us to simulate the supervised fine-tuning phase, similar to what is done in the development of models like ChatGPT. By using the Alpaca dataset, we hope to enhance the BLOOM-1.7B's ability to follow instructions and perform tasks as specified by users.
